---
permalink: /
title: "Hello. I'm Ilia."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm doing my PhD in Statistics at the University of Waterloo. I'm fascinated by deep learning and its ability to reach superhuman performance on so many different tasks. I want to better understand how neural networks achieve such impressive results... and why sometimes they don't. To do that, I'm exploring what kind of information or knowledge is contained in the datasets we train our models on and how much of this knowledge is actually needed for our models. 

In the beginning, I used deep learning to restore lost data from cars in order to improve anomaly dection algorithms and make cars safer. The ability to restore lost data suggests that knowledge is duplicated across a dataset. 

Now, I work on dataset distillation: the process of creating tiny synthetic datasets that contain all the knowledge of much larger datasets. If knowledge is duplicated across a dataset then it should be possible to represent all that knowledge using far fewer samples. 

Next, I want to work on creating a metric for objectively comparing the difficulty of machine learning tasks based on how much knowledge is needed to solve them. Knowing how tough our tasks are tells us a lot about how good our models are. 
