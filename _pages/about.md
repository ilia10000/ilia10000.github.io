---
permalink: /
title: "Hello. I'm Ilia."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm doing my PhD in Statistics at the University of Waterloo. I'm fascinated by deep learning and its ability to reach superhuman performance on so many different tasks. I want to better understand how neural networks achieve such impressive results... and why sometimes they don't. To do that, I'm exploring what kind of information or knowledge is contained in the datasets we train our models on and how much of this knowledge is actually needed for our models. 

In the beginning, I used deep learning to restore lost data from cars in order to improve anomaly dection algorithms and make cars safer. The ability to restore lost data suggests that knowledge is duplicated across a dataset. 

Then, I worked on improving dataset distillation: the process of learning tiny synthetic datasets that contain all the knowledge of much larger datasets. If knowledge is duplicated across a dataset then it should be possible to represent all that knowledge using far fewer samples. 

Now, I work on "less than one"-shot Learning, an extreme form of few-shot learning where the goal is for models to learn N new classes using M<N training samples. 

Check out my publications to see my progress so far. If you find something you're interested in discussing then shoot me an email and I'd be happy to chat. The best place to reach me is at isucholu@uwaterloo.ca.
